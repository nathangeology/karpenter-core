name: kind-e2e
on:
  push:
    branches: [main]
  workflow_dispatch:
jobs:
  kind-e2e:
    permissions:
      issues: write
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        k8sVersion: ["1.34.0"] # 1.30.x", "1.31.x",
        scenario: ["hack/e2e_driver/baseline_scenario_small"] # "hack/e2e_driver/kubernetes_scenario_1753908801",
        include:
#          - k8sVersion: "1.30.x"
#            kindNodeImage: "kindest/node:v1.30.4@sha256:976ea815844d5fa93be213437e3ff5754cd599b040946b5cca43ca45c2047114"
#            kubectlVersion: "v1.30.4"
#          - k8sVersion: "1.31.x"
#            kindNodeImage: "kindest/node:v1.31.1@sha256:de14b5d8e3c9cd0b576ca570b8f1b774b2df9b5d4930d86139b132b7a6ac7c5e"
#            kubectlVersion: "v1.31.1"
          - k8sVersion: "1.34.0"
            kindNodeImage: "kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a"
#            kubectlVersion: "v1.34.0"
    steps:
    - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
    - name: Verify scenarios exist
      run: |
        echo "Verifying scenario directories exist..."
        for scenario_dir in ${{ matrix.scenario }}; do
          if [ ! -d "$scenario_dir" ]; then
            echo "Error: Scenario directory does not exist: $scenario_dir"
            exit 1
          fi
          echo "âœ“ Found scenario directory: $scenario_dir"
        done
    - name: Set up Python 3.10
      uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c # v6.0.0
      with:
        python-version: "3.10"
    - uses: ./.github/actions/install-deps
      with:
          k8sVersion: ${{ matrix.k8sVersion }}
    - name: Kind Cluster
      uses: helm/kind-action@a1b0e391336a6ee6713a0583f8c6240d70863de3 # v1.12.0
      with:
        version: v0.30.0 #        config: hack/e2e_driver/kwok-config/kind-config.yaml
        node_image: ${{ matrix.kindNodeImage }}
        #        kubectl_version: ${{ matrix.kubectlVersion }}
        wait: 30s
        verbosity: 10
    - name: check kind cluster and taint nodes
      shell: bash
      run: |
        kubectl config current-context
        kubectl get nodes
        kubectl taint nodes chart-testing-control-plane CriticalAddonsOnly:NoSchedule
    - name: install prometheus 
      uses: ./.github/actions/install-prometheus
    - name: install pyroscope
      uses: ./.github/actions/install-pyroscope
    - name: install kwok and controller
      shell: bash
      run: |
        make install-kwok
        export KWOK_REPO=kind.local
        export KIND_CLUSTER_NAME=chart-testing
        make apply-with-kind
        # Fix kube-scheduler authentication and RBAC permissions
        chmod +x ./hack/e2e_driver/kwok-config/patch-scheduler-rbac.sh
        chmod +x ./hack/e2e_driver/kwok-config/fix-scheduler-auth.sh
        ./hack/e2e_driver/kwok-config/patch-scheduler-rbac.sh
        ./hack/e2e_driver/kwok-config/fix-scheduler-auth.sh
        
        # Nuclear option: Restart API server to clear authentication caches
        echo "=== Restarting API Server to Clear Caches ==="
        kubectl delete pod -n kube-system -l component=kube-apiserver
        echo "Waiting for API server to restart..."
        sleep 30
        kubectl get pods -n kube-system -l component=kube-apiserver
        
        # Verify the authentication fix worked
        echo "=== Post-Fix Authentication Test ==="
        kubectl auth can-i get configmaps --as=system:kube-scheduler -n kube-system || echo "ConfigMap access still broken"
        kubectl auth can-i get nodes --as=system:kube-scheduler || echo "Node access still broken"
        kubectl auth can-i get pods --as=system:kube-scheduler || echo "Pod access still broken"
        
        # Debug: Comprehensive Scheduler RBAC Diagnosis
        echo "=== Scheduler RBAC Diagnosis ==="
        echo "Default scheduler ClusterRoles:"
        kubectl get clusterroles | grep -i scheduler || echo "No scheduler ClusterRoles found"
        echo "Default scheduler ClusterRoleBindings:"
        kubectl get clusterrolebindings | grep -i scheduler || echo "No scheduler ClusterRoleBindings found"
        echo "Scheduler service accounts:"
        kubectl get serviceaccounts -n kube-system | grep -i scheduler || echo "No scheduler ServiceAccounts found"
        echo "Default system:kube-scheduler binding:"
        kubectl describe clusterrolebinding system:kube-scheduler || echo "Default scheduler binding missing"
        echo "Scheduler pod service account:"
        kubectl get pod -n kube-system -l component=kube-scheduler -o yaml | grep -A5 -B5 serviceAccount || echo "Could not get scheduler pod details"
        echo "All kube-system ClusterRoleBindings:"
        kubectl get clusterrolebindings -o wide | grep kube-system || echo "No kube-system ClusterRoleBindings found"
        
        # Debug: Authentication Debugging
        echo "=== Authentication Debugging ==="
        # Check what identity the scheduler is actually using
        kubectl auth whoami --as=system:kube-scheduler || echo "Cannot authenticate as system:kube-scheduler"
        
        # Check if the scheduler can access anything at all
        kubectl auth can-i get nodes --as=system:kube-scheduler || echo "system:kube-scheduler cannot get nodes"
        kubectl auth can-i get pods --as=system:kube-scheduler || echo "system:kube-scheduler cannot get pods"
        kubectl auth can-i list nodes --as=system:kube-scheduler || echo "system:kube-scheduler cannot list nodes"
        kubectl auth can-i list pods --as=system:kube-scheduler || echo "system:kube-scheduler cannot list pods"
        
        # Check the scheduler's kubeconfig
        echo "Scheduler kubeconfig (first 20 lines):"
        kubectl exec -n kube-system -l component=kube-scheduler -- cat /etc/kubernetes/scheduler.conf | head -20 || echo "Could not get scheduler kubeconfig"
        
        # Check if anonymous access is working
        kubectl auth can-i get nodes --as=system:anonymous || echo "Anonymous cannot get nodes"
        
        # Check current user permissions
        kubectl auth whoami || echo "Cannot determine current user"
        kubectl auth can-i get nodes || echo "Current user cannot get nodes"
        
        # Debug: Verify our RBAC was applied successfully
        echo "=== Our RBAC Verification ==="
        kubectl get clusterrole system:kube-scheduler-extended || echo "Our ClusterRole not found"
        kubectl get clusterrolebinding system:kube-scheduler-extended || echo "Our ClusterRoleBinding not found"
        kubectl get rolebinding -n kube-system kube-scheduler-extension-apiserver-authentication-reader || echo "Our RoleBinding not found"
        kubectl get role -n kube-system extension-apiserver-authentication-reader || echo "Built-in role not found"
        
        # Apply enhanced KWOK stages for better event generation
        kubectl apply -f ./hack/kwok/stages/pod-complete-lifecycle.yaml
        kubectl apply -f ./hack/kwok/stages/pod-container-events.yaml
        kubectl apply -f ./hack/kwok/stages/pod-terminating.yaml
        kubectl apply -f ./hack/kwok/stages/node-terminating.yaml
        # Apply additional KWOK node configuration
        kubectl apply -f ./hack/e2e_driver/kwok-config/fast_node.yaml
        kubectl apply -f ./hack/e2e_driver/kwok-config/node-heartbeat-with-lease.yml
        
        # Debug: Check scheduler pod before restart
        echo "=== Scheduler Pod Before Restart ==="
        kubectl get pods -n kube-system -l component=kube-scheduler
        
        # Restart kube-scheduler to pick up new permissions
        kubectl delete pod -n kube-system -l component=kube-scheduler
        
        # Wait for scheduler to restart and check status
        echo "=== Waiting for Scheduler Restart ==="
        sleep 10
        kubectl get pods -n kube-system -l component=kube-scheduler
        kubectl describe pod -n kube-system -l component=kube-scheduler

    - name: ping cluster
      shell: bash
      run: | 
        sleep 15
        kubectl get pods -n kube-system | grep karpenter 
        kubectl get nodepools
        kubectl get pods -A
        kubectl describe nodes
    # TEMPORARILY DISABLED: Karpenter KPI Analysis Package
    # This package analyzes key performance indicators (KPIs) for integration tests.
    # Currently non-functional due to changes in core Karpenter metrics.
    # Disabled due to test flakiness until underlying metric changes are addressed.
    # Reference: https://github.com/nathangeology/karpenter_evaluate/blob/main/main.py
    # - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
    #   with:
    #     repository: nathangeology/karpenter_evaluate
    #     path: ./karpenter_eval/ # Installs to a folder in the Karpenter repo for the test
    #     ref: "1130af927302e6913a4947952112f793eeafc564"
    #     fetch-depth: 0
    # - name: install KPI report dependencies
    #   shell: bash
    #   run: |
    #     pip install pandas==2.2.2
    #     pip install pyarrow==16.1.0 
    #     pip install tabulate==0.9.0
    #     pip install prometheus-api-client==0.5.5
    #     pip install ./karpenter_eval/
    - name: Run Scenario Driver
      env:
        SCENARIO: ${{ matrix.scenario }}
        NAMESPACE: default
        LOG_DIR: ./scenario-logs/${{ matrix.scenario }}
        S3_BUCKET: ${{ secrets.S3_BUCKET }}
        S3_REGION: us-east-1
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        kubectl get pods -A
        kubectl get nodes
        kubectl get nodepools
        ./hack/scenario-driver.sh
    #    - name: run test suites
    #      shell: bash
    #      run: |
    #        OUTPUT_DIR=$(mktemp -d)
    #        export OUTPUT_DIR
    #        echo OUTPUT_DIR="$OUTPUT_DIR" >> "$GITHUB_ENV"
    #        make e2etests
    # - name: run test analysis
    #   shell: bash
    #   run: |
    #     OUTPUT_DIR=${{ env.OUTPUT_DIR }} python ./karpenter_eval/main.py
    - name: cleanup 
      shell: bash
      run: | 
        echo "Cleaning up resources for scenario: ${{ matrix.scenario }}"
        kubectl delete nodepools --all || true
        kubectl delete kwoknodeclasses.karpenter.kwok.sh --all || true
        make delete || true
        make uninstall-kwok || true
