name: kind-e2e
on:
  push:
    branches: [main]
  workflow_dispatch:
jobs:
  kind-e2e:
    permissions:
      issues: write
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        k8sVersion: ["1.34.0"] # 1.30.x", "1.31.x",
        scenario: ["hack/e2e_driver/baseline_scenario_small"] #, "hack/e2e_driver/baseline_scenario_large", "hack/e2e_driver/stress_test_scenario"] # "hack/e2e_driver/kubernetes_scenario_1753908801",
        include:
#          - k8sVersion: "1.30.x"
#            kindNodeImage: "kindest/node:v1.30.4@sha256:976ea815844d5fa93be213437e3ff5754cd599b040946b5cca43ca45c2047114"
#            kubectlVersion: "v1.30.4"
#          - k8sVersion: "1.31.x"
#            kindNodeImage: "kindest/node:v1.31.1@sha256:de14b5d8e3c9cd0b576ca570b8f1b774b2df9b5d4930d86139b132b7a6ac7c5e"
#            kubectlVersion: "v1.31.1"
          - k8sVersion: "1.34.0"
            kindNodeImage: "kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a"
#            kubectlVersion: "v1.34.0"
    steps:
    - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
    - name: Verify scenarios exist
      run: |
        echo "Verifying scenario directories exist..."
        for scenario_dir in ${{ matrix.scenario }}; do
          if [ ! -d "$scenario_dir" ]; then
            echo "Error: Scenario directory does not exist: $scenario_dir"
            exit 1
          fi
          echo "✓ Found scenario directory: $scenario_dir"
        done
    - name: Set up Python 3.10
      uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c # v6.0.0
      with:
        python-version: "3.10"
    - uses: ./.github/actions/install-deps
      with:
          k8sVersion: ${{ matrix.k8sVersion }}
    - name: Kind Cluster
      uses: helm/kind-action@a1b0e391336a6ee6713a0583f8c6240d70863de3 # v1.12.0
      with:
        version: v0.30.0 #        config: hack/e2e_driver/kwok-config/kind-config.yaml
        node_image: ${{ matrix.kindNodeImage }}
        #        kubectl_version: ${{ matrix.kubectlVersion }}
        wait: 30s
        verbosity: 10
        config: hack/e2e_driver/kwok-config/kind-config.yml
    - name: check kind cluster and taint nodes
      shell: bash
      run: |
        kubectl config current-context
        kubectl get nodes
        kubectl taint nodes chart-testing-control-plane CriticalAddonsOnly:NoSchedule
    - name: install prometheus 
      uses: ./.github/actions/install-prometheus
    - name: install pyroscope
      uses: ./.github/actions/install-pyroscope
    - name: install kwok and controller
      shell: bash
      run: |
        make install-kwok
        export KWOK_REPO=kind.local
        export KIND_CLUSTER_NAME=chart-testing
        make apply-with-kind
        # Fix kube-scheduler with node-level admin permissions (nuclear option)
        chmod +x ./hack/e2e_driver/kwok-config/grant-scheduler-admin.sh
        chmod +x ./hack/e2e_driver/kwok-config/grant-node-admin.sh
        ./hack/e2e_driver/kwok-config/grant-scheduler-admin.sh
        ./hack/e2e_driver/kwok-config/grant-node-admin.sh
        
        # KIND-specific debugging
        echo "=== KIND Authentication Investigation ==="
        # Get scheduler pod name first
        SCHEDULER_POD=$(kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}')
        echo "Scheduler pod name: $SCHEDULER_POD"
        
        # Check scheduler pod configuration directly
        echo "Scheduler Pod Configuration:"
        kubectl get pod -n kube-system $SCHEDULER_POD -o yaml
        
        # Check scheduler pod description for volume mounts and authentication
        echo "Scheduler Pod Details:"
        kubectl describe pod -n kube-system $SCHEDULER_POD
        
        # Check kubeadm-config ConfigMap
        echo "kubeadm-config ConfigMap:"
        kubectl get configmap kubeadm-config -n kube-system -o yaml || echo "Could not get kubeadm-config"
        
        # Check from KIND node container directly
        echo "Checking from KIND node container:"
        docker exec chart-testing-control-plane cat /kind/kubeadm.conf || echo "Could not get kubeadm config from node"
        docker exec chart-testing-control-plane cat /etc/kubernetes/manifests/kube-scheduler.yaml || echo "Could not get scheduler manifest from node"
        docker exec chart-testing-control-plane ls -la /etc/kubernetes/pki/ || echo "Could not list certificates from node"
        
        # Check the scheduler.conf file content (the key authentication file)
        echo "Scheduler kubeconfig content:"
        docker exec chart-testing-control-plane cat /etc/kubernetes/scheduler.conf || echo "Could not get scheduler kubeconfig from node"
        
        # Check if the scheduler.conf file exists and has proper permissions
        echo "Scheduler kubeconfig file details:"
        docker exec chart-testing-control-plane ls -la /etc/kubernetes/scheduler.conf || echo "Could not check scheduler kubeconfig file"
        
        # Test the scheduler kubeconfig directly
        echo "Testing scheduler kubeconfig:"
        docker exec chart-testing-control-plane kubectl --kubeconfig=/etc/kubernetes/scheduler.conf auth whoami || echo "Scheduler kubeconfig authentication failed"
        docker exec chart-testing-control-plane kubectl --kubeconfig=/etc/kubernetes/scheduler.conf auth can-i get nodes || echo "Scheduler kubeconfig cannot get nodes"
        
        # Check scheduler logs after admin permissions granted
        echo "=== Scheduler Logs After Admin Permissions ==="
        kubectl logs -n kube-system -l component=kube-scheduler --tail=20 | grep -E "(forbidden|Failed to watch|Starting Kubernetes Scheduler)" || echo "No scheduler error logs found"
        
        # Wait for permissions to potentially take effect
        echo "Waiting for permissions to take effect..."
        sleep 10
        
        # Check scheduler logs again to see if errors reduced
        echo "=== Scheduler Logs After Wait Period ==="
        kubectl logs -n kube-system -l component=kube-scheduler --tail=20 | grep -E "(forbidden|Failed to watch)" || echo "No forbidden errors found"
        
        # Check scheduler logs after authentication fix
        echo "=== Scheduler Logs After Authentication Fix ==="
        kubectl logs -n kube-system -l component=kube-scheduler --tail=20 | grep -E "(forbidden|Failed to watch|authentication)" || echo "No authentication-related logs"
        
        # Nuclear option: Restart API server to clear authentication caches
        echo "=== Restarting API Server to Clear Caches ==="
        kubectl delete pod -n kube-system -l component=kube-apiserver
        echo "Waiting for API server to restart..."
        sleep 30
        kubectl get pods -n kube-system -l component=kube-apiserver
        
        # Check scheduler logs after API server restart
        echo "=== Scheduler Logs After API Server Restart ==="
        kubectl logs -n kube-system -l component=kube-scheduler --tail=20 | grep -E "(forbidden|Failed to watch|Caches are synced)" || echo "No cache-related logs"
        
        # Verify the authentication fix worked
        echo "=== Post-Fix Authentication Test ==="
        kubectl auth can-i get configmaps --as=system:kube-scheduler -n kube-system || echo "ConfigMap access still broken"
        kubectl auth can-i get nodes --as=system:kube-scheduler || echo "Node access still broken"
        kubectl auth can-i get pods --as=system:kube-scheduler || echo "Pod access still broken"
        
        # Debug: Comprehensive Scheduler RBAC Diagnosis
        echo "=== Scheduler RBAC Diagnosis ==="
        echo "Default scheduler ClusterRoles:"
        kubectl get clusterroles | grep -i scheduler || echo "No scheduler ClusterRoles found"
        echo "Default scheduler ClusterRoleBindings:"
        kubectl get clusterrolebindings | grep -i scheduler || echo "No scheduler ClusterRoleBindings found"
        echo "Scheduler service accounts:"
        kubectl get serviceaccounts -n kube-system | grep -i scheduler || echo "No scheduler ServiceAccounts found"
        echo "Default system:kube-scheduler binding:"
        kubectl describe clusterrolebinding system:kube-scheduler || echo "Default scheduler binding missing"
        echo "Scheduler pod service account:"
        kubectl get pod -n kube-system -l component=kube-scheduler -o yaml | grep -A5 -B5 serviceAccount || echo "Could not get scheduler pod details"
        echo "All kube-system ClusterRoleBindings:"
        kubectl get clusterrolebindings -o wide | grep kube-system || echo "No kube-system ClusterRoleBindings found"
        
        # Debug: Authentication Debugging
        echo "=== Authentication Debugging ==="
        # Check what identity the scheduler is actually using
        kubectl auth whoami --as=system:kube-scheduler || echo "Cannot authenticate as system:kube-scheduler"
        
        # Check if the scheduler can access anything at all
        kubectl auth can-i get nodes --as=system:kube-scheduler || echo "system:kube-scheduler cannot get nodes"
        kubectl auth can-i get pods --as=system:kube-scheduler || echo "system:kube-scheduler cannot get pods"
        kubectl auth can-i list nodes --as=system:kube-scheduler || echo "system:kube-scheduler cannot list nodes"
        kubectl auth can-i list pods --as=system:kube-scheduler || echo "system:kube-scheduler cannot list pods"
        
        # Check the scheduler's kubeconfig
        echo "Scheduler kubeconfig (first 20 lines):"
        kubectl exec -n kube-system -l component=kube-scheduler -- cat /etc/kubernetes/scheduler.conf | head -20 || echo "Could not get scheduler kubeconfig"
        
        # Check if anonymous access is working
        kubectl auth can-i get nodes --as=system:anonymous || echo "Anonymous cannot get nodes"
        
        # Check current user permissions
        kubectl auth whoami || echo "Cannot determine current user"
        kubectl auth can-i get nodes || echo "Current user cannot get nodes"
        
        # Debug: Node Authorization Investigation
        echo "=== Node Authorization Investigation ==="
        # Check node authorization settings
        kubectl get nodes -o yaml | grep -A10 -B10 authorization || echo "No node authorization config found"
        
        # Check bootstrap token RBAC
        echo "Bootstrap token ClusterRoleBindings:"
        kubectl get clusterrolebindings | grep bootstrap || echo "No bootstrap ClusterRoleBindings found"
        
        # Check node-specific RBAC
        echo "Node-related ClusterRoleBindings:"
        kubectl get clusterrolebindings | grep node || echo "No node ClusterRoleBindings found"
        
        # Check what node-specific RBAC exists
        echo "Node-specific RBAC details:"
        kubectl describe clusterrolebinding | grep -A20 -B5 node || echo "No node RBAC details found"
        
        # Check if there are node-specific service accounts
        echo "Node-related ServiceAccounts:"
        kubectl get serviceaccounts -n kube-system | grep node || echo "No node ServiceAccounts found"
        
        # Check API server authorization modes
        echo "API Server authorization configuration:"
        kubectl get pod -n kube-system -l component=kube-apiserver -o yaml | grep -A5 -B5 "authorization-mode" || echo "Could not get API server authorization config"
        
        # Check if Node authorization is enabled
        echo "Checking for Node authorization mode:"
        kubectl logs -n kube-system -l component=kube-apiserver --tail=100 | grep -i "authorization" || echo "No authorization logs in API server"
        
        # Debug: Verify our RBAC was applied successfully
        echo "=== Our RBAC Verification ==="
        kubectl get clusterrole system:kube-scheduler-extended || echo "Our ClusterRole not found"
        kubectl get clusterrolebinding system:kube-scheduler-extended || echo "Our ClusterRoleBinding not found"
        kubectl get rolebinding -n kube-system kube-scheduler-extension-apiserver-authentication-reader || echo "Our RoleBinding not found"
        kubectl get role -n kube-system extension-apiserver-authentication-reader || echo "Built-in role not found"
        
        # Apply enhanced KWOK stages for better event generation
        kubectl apply -f ./hack/kwok/stages/pod-complete-lifecycle.yaml
        kubectl apply -f ./hack/kwok/stages/pod-container-events.yaml
        kubectl apply -f ./hack/kwok/stages/pod-terminating.yaml
        kubectl apply -f ./hack/kwok/stages/node-terminating.yaml
        # Apply additional KWOK node configuration
        kubectl apply -f ./hack/e2e_driver/kwok-config/fast_node.yaml
        kubectl apply -f ./hack/e2e_driver/kwok-config/node-heartbeat-with-lease.yml
        
        # Check scheduler status without restarting (avoid losing permissions)
        echo "=== Scheduler Status After Admin Permissions ==="
        kubectl get pods -n kube-system -l component=kube-scheduler
        
        # Wait a moment for permissions to take effect, then check logs
        echo "Waiting for permissions to take effect..."
        sleep 15
        
        # Check if scheduler logs show improvement
        echo "=== Scheduler Logs After Admin Permissions ==="
        kubectl logs -n kube-system -l component=kube-scheduler --tail=30 | grep -E "(forbidden|Failed to watch|successfully|healthy)" || echo "No relevant scheduler logs"
        
        # Final verification before scenario
        echo "=== Final Scheduler Permissions Test ==="
        kubectl auth can-i "*" "*" --as=system:kube-scheduler && echo "✅ Scheduler has admin access" || echo "❌ Admin access failed"

    - name: ping cluster
      shell: bash
      run: | 
        sleep 15
        kubectl get pods -n kube-system | grep karpenter 
        kubectl get nodepools
        kubectl get pods -A
        kubectl describe nodes
    # TEMPORARILY DISABLED: Karpenter KPI Analysis Package
    # This package analyzes key performance indicators (KPIs) for integration tests.
    # Currently non-functional due to changes in core Karpenter metrics.
    # Disabled due to test flakiness until underlying metric changes are addressed.
    # Reference: https://github.com/nathangeology/karpenter_evaluate/blob/main/main.py
    # - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
    #   with:
    #     repository: nathangeology/karpenter_evaluate
    #     path: ./karpenter_eval/ # Installs to a folder in the Karpenter repo for the test
    #     ref: "1130af927302e6913a4947952112f793eeafc564"
    #     fetch-depth: 0
    # - name: install KPI report dependencies
    #   shell: bash
    #   run: |
    #     pip install pandas==2.2.2
    #     pip install pyarrow==16.1.0 
    #     pip install tabulate==0.9.0
    #     pip install prometheus-api-client==0.5.5
    #     pip install ./karpenter_eval/
    - name: Run Scenario Driver
      env:
        SCENARIO: ${{ matrix.scenario }}
        NAMESPACE: default
        LOG_DIR: ./scenario-logs/${{ matrix.scenario }}
        S3_BUCKET: ${{ secrets.S3_BUCKET }}
        S3_REGION: us-east-1
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        kubectl get pods -A
        kubectl get nodes
        kubectl get nodepools
        ./hack/scenario-driver.sh
    #    - name: run test suites
    #      shell: bash
    #      run: |
    #        OUTPUT_DIR=$(mktemp -d)
    #        export OUTPUT_DIR
    #        echo OUTPUT_DIR="$OUTPUT_DIR" >> "$GITHUB_ENV"
    #        make e2etests
    # - name: run test analysis
    #   shell: bash
    #   run: |
    #     OUTPUT_DIR=${{ env.OUTPUT_DIR }} python ./karpenter_eval/main.py
    - name: cleanup 
      shell: bash
      run: | 
        echo "Cleaning up resources for scenario: ${{ matrix.scenario }}"
        kubectl delete nodepools --all || true
        kubectl delete kwoknodeclasses.karpenter.kwok.sh --all || true
        make delete || true
        make uninstall-kwok || true
